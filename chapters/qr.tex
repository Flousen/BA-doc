\chapter{QR-Zerlegung}

\section{Definition}
Eine Matrix $A \in \mathbb{R}^{m \times n}$ , $m \ge n$ besitzt eine eindeutige QR-Zerlegung.
\begin{align}
	A = QR
\end{align}
mit einer orthogonalen Matrix $ Q \in \mathbb{R}^{m \times m} $ und einer oberen Dreiecksmatrix $ R \in \mathbb{R}^{n \times n}$ \cite{num1}

Eine QR Zerlegung kann mit einer Householder-Transformation berechnet werden.

\subsection{Beispiel}
Lösung eines Minimierungsproblem
\begin{align}
	\min_{x \in \mathbb{R}^n} \|Ax-b\|^2 \label{eq1}
\end{align}
mit Matrix $A \in \mathbb{R}^{m\times n}$ mit $rang(A) = n < m$ für die eine QR Zerlegung existiert.
$R$ besitzt die Gestalt 
\begin{align*}
	R=	
	\left(\begin{array}{ccc}
		*&*&* \\ 
		&*&* \\ 
		& &* \\ \hline
		& 0 &
	\end{array} \right)
	=
	\left(\begin{array}{c}
	 \\ 
	\hat{R} \\ 
	 \\ \hline
	0
	\end{array} \right) 
\end{align*}

$\hat{R}$ stellt eine obere Dreiecksmatrix dar.
Damit kann man das Minimierungs Problem wie folgt modifizieren mit $A=QR$
\begin{align}
		\min_{x \in \mathbb{R}^n} \|Ax-b\|^2 =
		\min_{x \in \mathbb{R}^n} \|Q^T(Ax-b)\|^2 =
		\min_{x \in \mathbb{R}^n} \|Rx-Q^Tb\|^2
\end{align}
Also löst
\begin{align}
Rx=Q^Tb \label{solvminqr}
\end{align}
das Minimierungsproblem (\ref{eq1}). Da $R$ eine Dreiecksmatrix ist, lässt sich (\ref{solvminqr}) leicht mit Rückwärtseinsetzen  lösen.

1\section{Householder-Transformation}
Sei  $v \in \mathbb{R}^n$ ein Vektor dann wird die $n \times n$ Matrix 
\begin{align}
	H = I - 2 \dfrac{vv^T}{v^Tv}
\end{align}
als Householder-Transformation und der Vektor $v$ als Householder-Vektor bezeichnet.
Eine Householder-Transformation $H = I - 2 \dfrac{vv^T}{v^Tv}$ ist orthogonal und symmetrisch. \cite{num1}\\
Die Householder-Transformation spiegelt den Vektor $x$ auf die Achse $x_1$.
Dazu multipliziert man $H$ von links auf $x$
\begin{align}
	Hx=\alpha e_1 \label{spiegelung}
\end{align}
mit dem Skalar $\alpha \in \mathbb{R}$ und $e_1$ als ersten kanonischen Einheitsvektor. Der Householder-Vektor steht senkrecht auf der Ebene an welcher $x$ gespiegelt wird.\\
Die Abbildung \ref{fig:HHolder} veranschaulicht die Spiegelung des Vektors $x$ an der gestrichelt eingezeichneten Ebene auf die Achse $x_1$.
[Abbildung \ref{fig:HHolder}]
\begin{figure}[h]
	\centering
	\input{images/hh.tex}
	\caption{Beispiel Householder-Transformation mit $x=(-1,2)^T$}
	\label{fig:HHolder}
\end{figure}

Eine Householder-Transformation kann die eine Matrix $A$ wie folgt transformieren.
\begin{align*}
	H_1 A= \left( 
	\begin{array}{cccc}
	* & * & * & * \\ 
	0 & * & * & * \\ 
	0 & * & * & * \\ 
	0 & * & * & *
	\end{array}
	\right)
	\quad , \quad
	H_2 H_1 A= \left( 
	\begin{array}{cccc}
	* & * & * & * \\ 
	0 & * & * & * \\ 
	0 & 0 & * & * \\ 
	0 & 0 & * & *
	\end{array}
	\right)
\end{align*} 
So erhält man folgende Faktorisierung
\begin{align*}
	R = H_{n-1} H_{n-2}\cdot ...\cdot H_1 A \Leftrightarrow A = (H_1\cdot ...\cdot H_n)R \Rightarrow Q = H_1\cdot ... \cdot H_n
\end{align*}
$Q$ ist also das Produkt aller Householder-Transformationen.


\subsection{Householder Vector}
%Wie muss der Vektor $v$ aussehen damit (\ref{spiegelung}) gilt?\\
Damit (\ref{spiegelung}) gilt, muss der Vektor folgendermaßen berechnet werden. \\
\glqq Mit $Hx = x - 2\dfrac{vv^T}{v^Tv} x = x - \lambda v \overset{!}{=} \alpha e_1$ folgt $v \in \text{span}\{x - \alpha e_1\}$. \grqq{} \cite{num1}Warum?\\
Die Definition des Vektors $v = x - \alpha e_1$ wird in $Hx = \alpha e_1 $ eingesetzt
\begin{align*}
	Hx =& x - \dfrac{2}{v^Tv}v(v^Tx) = x - 2\dfrac{v^Tx}{v^Tv}v\\
	=& x - \dfrac{(x - \alpha e_1)^Tx}{\|x - \alpha e_1\|_2^2} (x - \alpha e_1)
	=\underbrace{\left(1 - \dfrac{2(x - \alpha e_1)^Tx}{\|x - \alpha e_1\|_2^2}\right)}_{ \overset{!}{=} 0 } x + \alpha e_1 \underbrace{\dfrac{2(x - \alpha e_1)^Tx}{\|x - \alpha e_1\|_2^2} }_{\overset{!}{=} 1} \overset{!}{=} \alpha e_1
\end{align*}

Damit das Ergebnis $ = \alpha e_1$ erhalten wird, muss gelten
%Damit das letzte $=$ gilt muss 
\begin{align*}
	1 &= \dfrac{2(x - \alpha e_1)^Tx}{\|x - \alpha e_1\|^2}\\
	\Leftrightarrow (x - \alpha e_1)^T(x - \alpha e_1) &= 2 x^T x - 2\alpha x_1 \\
	\Leftrightarrow x^Tx -2\alpha x_1 + \alpha^2 &= 2 x^T x - 2\alpha x_1\\
	\Leftrightarrow \alpha &= \pm \sqrt{x^Tx}
\end{align*}

Das Vorzeichen von  $\alpha = \pm \sqrt{x^Tx}$ kann man frei wählen, um $ v = x - \alpha e_1$ zu berechnen.


%Wie ist das Vorzeichen von $\alpha = \pm \sqrt{x^Tx}$ zu wählen um $ v = x - \alpha e_1$ zu berechnen?

Wählt man das Vorzeichen positiv kann Auslöschung auftreten, falls $x$ annähernd ein positives Vielfaches von $e_1$ ist.

LAPACK \cite{DGEQR2} vermeidet die Auslöschung indem das Vorzeichen entgegengesetzt gewählt wird. Das bedeutet $x$ wird immer auf die gegenüberliegende Seite gespiegelt.

Im Skript von Numerik 1 \cite{num1} wird das Vorzeichen immer positiv gewählt\\ $\alpha = |\sqrt{x^Tx}| = \|x\|_2$. Eine mögliche Auslöschung im Fall $ x_1 > 0$ wird hier durch die folgende Umformung vermieden.
\begin{align*}
	v_1 = x_1 - \|x\|_2 = \dfrac{x_1^2 - \|x\|_2^2}{x_1 + \|x\|_2}
	=\dfrac{-(x_2^2+...+x_n^2)}{x_1 + \|x\|_2}
\end{align*}


Der Vorteil bei der von LAPACK verwendeten Methode ist, dass hier nur die Norm berechnet werden muss, wohingegen bei dem anderen Algorithmus das Skalarprodukt $x^Tx$ berechnet werden muss. Dies kann bei Vektoren mit vielen Einträgen und großen Werten zu einem Überlauf führen. Es muss jedoch ein Algorithmus gewählt werden, der die Norm berechnet $\|x\|=\sqrt{x^Tx}$ ohne das Skalarprodukt explizit auszurechnen.

Um den Vektor $v$ später auf der frei werdenden Diagonalen von $A$ speichern zu können wird er auf $v_1 = 1$ normiert. Dies geschieht mit 
\begin{align*}
	v = \dfrac{x - \alpha e_1}{x_1 - \alpha}
\end{align*}

Mit der Normierung kann man den Faktor $\tau = \dfrac{2}{v^Tv}$ berechnen.
\begin{align*}
	\tau = \dfrac{2}{v^Tv} = \dfrac{2 (x_1 - \alpha)^2}{(x - \alpha e_1)^T (x - \alpha e_1)} = \dfrac{2 (x_1 - \alpha)^2}{\|x\|^2_2 - 2\alpha x^Te_1 + \alpha^2} =  \dfrac{2 (x_1 - \alpha)^2}{ 2\alpha (\alpha - x_1)} = \dfrac{x_1 - \alpha}{\alpha}
\end{align*}

Mit dem Faktor $\tau = \dfrac{2}{v^Tv}$ kann man die Householder-Transformation schreiben als
\begin{align*}
	H = I - 2 \dfrac{vv^T}{v^Tv} = I - \tau v v^T
\end{align*}

Das macht man, weil die Berechnung des Skalarprodukts relativ aufwändig ist. Da man das Skalarprodukt zur Berechnung des Householder-Vektors benötigt, kann man damit direkt den Faktor $\tau$ berechnen.


\begin{algorithm}
	\caption{Housholder-Vector(LAPACK DLARFG)}
	\begin{algorithmic}
		\State Input: $x \in \mathbb{R}^n$ 
		\State $\alpha = -1 * \text{sign}(x_1) \|x\|_2$
		\State $\tau = \frac{x_1 - \alpha}{\alpha}$
		\State $v=\frac{x - \alpha e_1}{x_1 - \alpha}$
		\State Output: Householder-Vektor $v$ , $\tau$
	\end{algorithmic} 
	\label{alg:unblockedqr}
\end{algorithm}


\subsection{Householder-Transformation anwenden}
Ein aufwändiges Matrix-Matrix-Produkt kann bei der Anwendung der Housholder-Matrix $H = I - \tau vv^T$ auf die Matrix $A$ umgangen werden, indem man geschickt klammert.
\begin{align*} 
H A =(I - \tau vv^T) A= A - \tau (vv^T )A = A - \tau v*(v^T*A)
\end{align*}
Statt eines Matrix-Matrix-Produkts muss man nur ein Matrix-Vektor-Produkt und ein dyadisches Produkt berechnen.
Das Matrix-Vektor-Produkt und das dyadische Produkt haben nur einen Aufwand von $O(n^2)$.

Das führt zum Algorithmus \ref{alg:unblockedqr}. 
\begin{algorithm}
	\caption{Ungeblockte Housholder-Transformation}
	\begin{algorithmic}
	\State Input: $A \in \mathbb{R}^{m \times n}$
	\For {i = 0 : n}
		\State [$v$, $\tau$] = housevector($A(i:m,i)$)
		\State $w \leftarrow v^T*A(i:m,i:n)$ (dgemv)
		\State $ A(i:m,i:n) \leftarrow \tau * v * w + A $ (dger)
		\If {i > m}
			\State $A(i + 1 : m, i) \leftarrow v(2 : m - i + 1)$
		\EndIf
	\EndFor	
	\State Output: $A$ QR zerlegt, Vektor $\tau \in \mathbb{R}^n$
\end{algorithmic} 
\label{alg:unblockedqr}
\end{algorithm}



Der Algorithmus \ref{alg:unblockedqr} überschreibt die Matrix $A$ mit $R$. Da $R$ eine obere Dreiecksmatrix ist, werden unter der Diagonale die Housholder-Vektoren gespeichert. Da die Householder-Vektoren auf $v_1 = 1$ normiert wurden, muss das erste Element des Vektors nicht mit gespeichert werden. Die Householder-Vektoren können dadurch unterhalb der Diagonalen gespeichert werden.
Die Matrix $A$ hat also die Form
\begin{align*}
	A = 
	\left(\begin{array}{ccc}
	R   &  R  & R \\ 
	v_1 &  R  & R \\ 
	v_1 & v_2 & R \\ 
	v_1 & v_2 & v_3
	\end{array} \right)
\end{align*}


\section{Geblockte QR-Zerlegung}
Ein geblockter Algorithmus ist sinnvoll, um bei großen Matrizen den Cache optimal zu nutzen.

Die Matrix $A \in \mathbb{R}^{m \times n}$ wird geblockt, mit einer geeigneten Blockgröße $bs$, betrachtet.
\begin{align}
	A = \left(\begin{array}{l|l}
	A_{0, 0} & A_{0, \text{bs}} \\ \hline
	A_{\text{bs}, 0}   & A_{\text{bs}, \text{bs}} 	
	\end{array} \right) \label{equ:blockA}
\end{align}
Die Abbildung \ref{fig:blockA} zeigt schematisch die Partitionierung von A.

Nun wird eine QR-Zerlegung für den Block $ \left(\dfrac{A_{0, 0}}{A_{\text{bs}, 0}} \right)$
mit dem ungeblockten Algorithmus \ref{alg:unblockedqr} berechnet.
\begin{align}
	\left(\begin{array}{l} 
	A_{0, 0} \\ \hline
	A_{\text{bs}, 0}
	\end{array}\right)
	\leftarrow
	\left(\begin{array}{l} 
	Q_{0, 0}  \backslash R_{0,0} \\ \hline
	Q_{\text{bs}, 0} 
	\end{array}\right)
\end{align}

Im Block $A_{0, 0}$ steht auf und über der Diagonalen $R_{0,0}$. Unterhalb der Diagonalen und im Block $A_{\text{bs}, 0}$ stehen die Householder-Vektoren.

Aus den Householder-Vektoren wird eine obere Dreiecksmatrix $T \in \mathbb{R}^{bs \times bs}$ für die gilt $H_{0}\cdot ... \cdot H_{bs} = H = I - V*T*V^T$ berechnet.\\
$H^T$ wird auf $A_{0, \text{bs}}$ und $ A_{0,\text{bs}}$ angewendet.
\begin{align}
	\left(\begin{array}{l} 
	A_{0, \text{bs}} \\ \hline
	A_{bs, \text{bs}}
	\end{array}\right)
	\leftarrow
	H^T \left(\begin{array}{l} 
	A_{0, \text{bs}} \\ \hline
	A_{bs, \text{bs}}
	\end{array}\right)
\end{align}

%Betrachte nun den Block $A_{bs, \text{bs}}$ wie in (\ref{equ:blockA}), in der Abbildung \ref{fig:blockA} gestrichelt Dargestellt.

Der Block $A_{bs, \text{bs}}$ wird erneut aufgeteilt. Das ist in Abbildung \ref{fig:blockA} gestrichelt dargestellt.

[Abbildung \ref{fig:blockA}]
\begin{figure}
	\centering
	\input{images/partitionOFA1.tex}
	\caption{Partitionierung vom A}
	\label{fig:blockA}
\end{figure}

\subsection{Calc Factor T larft}

Die Funkton bekommt eine Dreiecksmatrix $V \in \mathbb{R}^{m \times k}$ einen Vektor $\tau \in \mathbb{R}^k$ und eine Matrix $T\in \mathbb{R}^{k\times k}$ übergeben. 
Die Funktion berechnet eine Dreiecksmatrix $T$ so dass
\begin{align*}
	H_1H_2...H_k = I - VTV^T \qquad \text{mit}\qquad H_i = I - \tau_i v_iv_i^T
\end{align*}

Warum und wie das Funktoniert wird hier beschreiben \cite{Joffrain:2006:AHT:1141885.1141886}.




\subsection{Apply H larfb}

Die Funktion larfb bekommt eine Dreiecksmatrix $V \in \mathbb{R}^{m \times k}$, eine Dreiecksmatrix $T \in \mathbb{R}^{k \times k}$ und eine Matrix $C \in \mathbb{R}^{m \times n }$ übergeben.
Die Funktion wendet eine  Block Reflector Matrix $ 	H = C - V T V^T $ von rechts auf die Matrix $ C $ an. 
Mit einem weiteren Übergabeparameter kann angeben werden ob die Block Reflector Matrix noch transponiert werden soll.
Die Funktion berechnet also
\begin{align*}
	C \leftarrow H C = C - V T V^T C \quad \text{oder} \quad 	C \leftarrow H^T C = C - V T^T V^T C
\end{align*}
Die Abbildung \ref{fig:patrA} zeigt die Partitionierung der Matrix A für die Funktion larfb.\\
Falls $m > k $ werden die Matrizen $V$ und $C$ aufgeteilt in $V=\left(\dfrac{V_1}{V_2}\right)$ und $C=\left(\dfrac{C_1}{C_2}\right)$. Dabei wird $V$ genau so gewählt, dass $V_1 \in \mathbb{R}^{k\times k}$ der Dreiecksteil der Matrix und quadratisch ist und $V_2 \in \mathbb{R}^{m-k\times k}$ der Rest der Matrix. Die Matrix $C$ wird in $C_1 \in \mathbb{R}^{k \times n}$ und $C_2 \in \mathbb{R}^{m-k \times n}$  aufgeteilt.\\
Die Aufteilung ist Notwendig da die BLAS-Funktion trmm (matrix-matrix product where one input matrix is triangular) nur für Quadratische Dreiecksmatrizen implementiert ist.

Im Fall $ m = k $ ist die Aufteilung nicht Notwendig da $ V $ quadratisch ist.

\begin{align*}
	(C_1^T * V_1 * T *V_1^T)^T\\
	V_1 * T^T * V_1^T * C_1
\end{align*}



Dies führt zu dem[ Algorithmus \ref{alg:applyblockref} ]\\
\begin{algorithm}
	\caption{Block reflector anwenden}
	\label{alg:applyblockref}
	\begin{algorithmic}
		\State 	$W \leftarrow C_1^T$ (copy)
		\State	$W \leftarrow W * V_1 $ (trmm)
		\If {m > k}
			\State $W \leftarrow W + C_2^T*V_2$ (gemm)
		\EndIf
		\State 	$ W \leftarrow W * T^T \quad \text{or}\quad  W * T$ (trmm)
		\If {m > k}
			\State $C_2 \leftarrow C_2 - V_2 * W^T$ (gemm)
		\EndIf
		\State 	$ W \leftarrow W * V_1^T $ (trmm)
		\State 	$ C_1 \leftarrow C_1 - W^T $
	\end{algorithmic}
\end{algorithm}


[Abbildung \ref{fig:patrA}]
\begin{figure} 
	\centering
	\input{images/partitionOFA.tex}
	\caption{Partitionierung vom A für larfb}
	\label{fig:patrA}
\end{figure}


\subsection{Iterativer Algorithmus}
[Algorithmus \ref{alg::italg}]
\begin{algorithm}
	\caption{Iterativer Algorithmus}
	\label{alg::italg}
	\begin{algorithmic}
		\For {i = 0 : n}
			\State QR = A;
			\If {i + ib > n}
				\State Calc T: $H=I-VTV^T$
				\State Apply H: $A=H^TA$
			\EndIf
		\EndFor
	\end{algorithmic}
\end{algorithm}
